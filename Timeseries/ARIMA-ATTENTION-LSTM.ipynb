{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f5b2f8",
   "metadata": {},
   "source": [
    "# ARIMA + LSTM Architectures on residuals\n",
    "\n",
    "    1) 'Vanilla' LSTM\n",
    "    2) Stacked LSTM\n",
    "    3) Bidirectional LSTM\n",
    "    4) Encoder-Decoder LSTM-LSTM\n",
    "    5) Encoder-Decoder CNN-LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c23bd",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "- Train the model on several stocks and try to predict one of them\n",
    "- Cluster stocks by correlation - make a 2D projection in order to use in Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import timeseries as ts\n",
    "\n",
    "def get_data():\n",
    "    sentic = pd.read_csv('sentic.csv', index_col=0, parse_dates=True)\n",
    "    sentic.index.freq='D'\n",
    "    sentic.to_excel('sentic.xlsx', sheet_name='Sentiment Analysis')\n",
    "    \n",
    "    btc = yf.download('BTC-USD', start='2020-02-14', end='2022-09-23', period='1d')[['Close']]\n",
    "    btc.columns = ['btc']\n",
    "    btc.index = sentic.index\n",
    "    btc.to_excel('btc.xlsx', sheet_name='BTC Closing Price')\n",
    "\n",
    "    #     sentic.index = pd.to_datetime(sentic.index).tz_localize('UTC')\n",
    "    data = pd.concat([sentic,btc], axis=1)\n",
    "    return data, sentic, btc\n",
    "\n",
    "# data, sentic, btc = get_data()\n",
    "\n",
    "sentiment = sentic['rate'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lags(df, n_lags=1, lead_time=1):\n",
    "    \"\"\"\n",
    "    Compute lags of a pandas.DataFrame from lead_time to lead_time + n_lags. Alternatively, a list can be passed as n_lags.\n",
    "    Returns a pd.DataFrame resulting from the concatenation of df's shifts.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(n_lags,int):\n",
    "        lag_list = range(lead_time, n_lags+lead_time)\n",
    "    else:\n",
    "        lag_list = n_lags\n",
    "    \n",
    "    lags=list()\n",
    "    for i in lag_list:\n",
    "        df_lag = df.shift(i)\n",
    "        if i!=0:                \n",
    "            df_lag.columns = [f'{col}_lag_{i}' for col in df.columns]\n",
    "        lags.append(df_lag) \n",
    "\n",
    "    return pd.concat(lags, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def add_dim(df, timesteps=5):\n",
    "    \"\"\"\n",
    "    Transforms a pd.DataFrame into a 3D np.array with shape (n_samples, timesteps, n_features)\n",
    "    \"\"\"\n",
    "    df = np.array(df)\n",
    "    array_3d = df.reshape(df.shape[0],timesteps ,df.shape[1]//timesteps)\n",
    "    return array_3d     \n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(df, target_name, n_lags, n_steps, lead_time, test_size, normalize=True, arima={order=(2,1,0),seasonal=(10,0,0,0)}):\n",
    "    '''\n",
    "    Prepare data for LSTM. \n",
    "    '''\n",
    "    \n",
    "    if arima:\n",
    "        from statsmodels.tsa.arima_model import ARIMA\n",
    "        arima = ARIMA(df[target_name], order=arima).fit()\n",
    "        residuals = arima.residuals\n",
    "        df.insert(df.columns.get_loc(target_name)+1, 'ARIMA_resid', residuals)\n",
    "    \n",
    "    if isinstance(n_steps,int):\n",
    "        n_steps = range(1,n_steps+1)\n",
    "    \n",
    "    n_steps_reverted = [-x for x in list(n_steps)]\n",
    "\n",
    "    X = make_lags(df, n_lags=n_lags, lead_time=lead_time).dropna()\n",
    "    y = make_lags(df[[target_name]], n_lags=n_steps_reverted).dropna()\n",
    "    \n",
    "    last_row = X.iloc[-1:,:]\n",
    "    \n",
    "    X, y = X.align(y, join='inner', axis=0)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "    if normalize:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        mms = MinMaxScaler().fit(X_train)\n",
    "        X_train, X_val = mms.transform(X_train), mms.transform(X_val)    \n",
    "        last_row = mms.transform(last_row)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    if isinstance(n_lags,int):\n",
    "        timesteps = n_lags\n",
    "    else:\n",
    "        timesteps = len(n_lags)\n",
    "    \n",
    "    return add_dim(X_train, timesteps), add_dim(X_val, timesteps), y_train, y_val, add_dim(last_row, timesteps)\n",
    "\n",
    "\n",
    "\n",
    "epochs, batch_size, verbose = 50, 72, 0\n",
    "\n",
    "model_params={}\n",
    "\n",
    "def fit_model(model, learning_rate=0.001, time_distributed=False, epochs=epochs, batch_size=batch_size, verbose=verbose, model_params=model_params):\n",
    "    \n",
    "    y_ind = y_val.index\n",
    "    \n",
    "    if time_distributed:\n",
    "        y_train_0 = y_train.to_numpy().reshape((y_train.shape[0], y_train.shape[1],1))\n",
    "        y_val_0 = y_val.to_numpy().reshape((y_val.shape[0], y_val.shape[1],1))        \n",
    "    \n",
    "    else:\n",
    "        y_train_0 = y_train\n",
    "        y_val_0 = y_val\n",
    "    \n",
    "    # fit network\n",
    "    from keras.optimizers import Adam\n",
    "    adam = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    \n",
    "    history = model.fit(X_train, y_train_0, \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size, \n",
    "                        verbose=verbose,\n",
    "                        **model_params, \n",
    "                        validation_data=(X_val, y_val_0), \n",
    "                        shuffle=False) \n",
    "\n",
    "    # make predictions and forecast\n",
    "    if time_distributed:\n",
    "        predictions = model.predict(X_val)[:,:,0]\n",
    "        future_forecast = model.predict(last_row)[:,:,0]\n",
    "    else:\n",
    "        predictions = model.predict(X_val)\n",
    "        future_forecast = model.predict(last_row)\n",
    "\n",
    "    # preparing for plot\n",
    "    yhat = pd.DataFrame(predictions, index=y_ind, columns=[f'pred_lag_{i}' for i in range(-n_steps,0)])\n",
    "    yhat_shifted = pd.concat([yhat.iloc[:,i].shift(-n_steps+i) for i in range(len(yhat.columns))], axis=1)\n",
    "\n",
    "\n",
    "    future_index = pd.date_range(start=y_val.index[-1]+pd.DateOffset(1+n_steps), periods=n_steps)\n",
    "    future_forecast = pd.DataFrame(future_forecast[0], index=future_index)\n",
    "    \n",
    "    # saving forecast and model architecture\n",
    "    name = model.name\n",
    "    pd.concat([btc.squeeze(),future_forecast]).to_excel(name+'_forecast.xlsx', sheet_name=name+' Forecast')\n",
    "    model.save(name+'model.h5')\n",
    "    \n",
    "    from keras.utils import plot_model\n",
    "    img_file = name+'_arch.png'\n",
    "    plot_model(model, to_file=img_file, show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    \n",
    "    # calculate RMSE\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, yhat))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(2,1,figsize=(14,14))\n",
    "\n",
    "    y_val.iloc[:,0].plot(ax=ax2,legend=True)\n",
    "    yhat_shifted.plot(ax=ax2, alpha=.2)\n",
    "    yhat_shifted.iloc[:,0].plot(ax=ax2, color='red')\n",
    "    ax2.set_title('Prediction comparison')\n",
    "    ax2.annotate(f'RMSE: {rmse:.5f} \\n R2 score: {r2_score(yhat,y_val):.5f}', xy=(.68,.93),  xycoords='axes fraction')\n",
    "\n",
    "    ax1.plot(history.history['loss'], label='train')\n",
    "    ax1.plot(history.history['val_loss'], label='test')\n",
    "    ax1.legend()\n",
    "\n",
    "    plt.show()\n",
    "    return model, future_forecast\n",
    "\n",
    "def merge_forecasts():\n",
    "    from glob import glob\n",
    "    filenames = glob(\"*_forecast.xlsx\")\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for name in filenames: \n",
    "        dfs.append(pd.read_excel(name, index_col=0))\n",
    "    \n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df.columns = [name.rstrip(r'_forecast.xlsx') for name in filenames]\n",
    "    df.to_excel('forecast_all.xlsx', sheet_name='DL Forecast')\n",
    "    return df, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bfeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604aef0a",
   "metadata": {},
   "source": [
    "# Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41bd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def create_output():\n",
    "    output = Sequential()\n",
    "    output.add(Dropout(.2, name='Dropout_1'))\n",
    "    output.add(Dense(100, name='Dense_1'))\n",
    "    output.add(Dropout(.2, name='Dropout_2'))\n",
    "    output.add(Dense(n_steps, name='Dense_2'))\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_lags, n_steps, lead_time, test_size = 11, 11, 0, .2\n",
    "\n",
    "epochs, batch_size, verbose = 100, 72, 0\n",
    "\n",
    "model_params = {'callbacks':[EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    mode=\"auto\")]}\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val, last_row = prepare_data(data, 'btc', n_lags, n_steps, lead_time, test_size)\n",
    "\n",
    "\n",
    "vanilla = Sequential(name='Vanilla')\n",
    "vanilla.add(LSTM(units=200, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2]) ))\n",
    "# vanilla.add(Dropout(.2, name='Dropout_1'))\n",
    "# vanilla.add(Dense(100, name='Dense_1'))\n",
    "# vanilla.add(Dropout(.2, name='Dropout_2'))\n",
    "# vanilla.add(Dense(n_steps, name='Dense_2'))\n",
    "vanilla.add(create_output())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fit_model(vanilla, learning_rate=0.001, time_distributed=False, epochs=epochs, batch_size=batch_size, verbose=verbose, model_params=model_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ebdc6",
   "metadata": {},
   "source": [
    "# Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = Sequential(name = 'Stacked')\n",
    "stacked.add(LSTM(units=100, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2]), return_sequences=True ))\n",
    "stacked.add(LSTM(units=200, activation='relu' ))\n",
    "stacked.add(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(stacked, learning_rate=0.001, time_distributed=False, epochs=epochs, batch_size=batch_size, verbose=verbose, model_params=model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b0740",
   "metadata": {},
   "source": [
    "# Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeddd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "bilstm = Sequential(name='Bidirectional')\n",
    "bilstm.add(Bidirectional(LSTM(100, activation='relu'), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bilstm.add(Sequential([\n",
    "    Dropout(.2, name='Dropout_1'),\n",
    "    Dense(100, name='Dense_1'),\n",
    "    Dropout(.2, name='Dropout_2'),\n",
    "    Dense(n_steps, name='Dense_2')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb485c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model(bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae8ad1",
   "metadata": {},
   "source": [
    "# Bidirectional - shorter train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "n_lags, n_steps, lead_time, test_size = 11, 11, 0, .2\n",
    "\n",
    "epochs, batch_size, verbose = 100, 72, 0\n",
    "\n",
    "model_params = {'callbacks':[EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    mode=\"auto\")]}\n",
    "\n",
    "\n",
    "data1 = data.loc['2022-05-15':,:] #.drop(columns=['neg_median','last','median'])\n",
    "\n",
    "X_train, X_val, y_train, y_val, last_row = prepare_data(data1, 'btc', n_lags, n_steps, lead_time, test_size)\n",
    "\n",
    "\n",
    "bilstm_short = Sequential(name='Bidirectional_Short')\n",
    "bilstm_short.add(Bidirectional(LSTM(100, activation='relu'), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bilstm_short.add(Sequential([\n",
    "    Dropout(.2, name='Dropout_1'),\n",
    "    Dense(100, name='Dense_1'),\n",
    "    Dropout(.2, name='Dropout_2'),\n",
    "    Dense(n_steps, name='Dense_2')\n",
    "]))\n",
    "\n",
    "fit_model(bilstm_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50363ac",
   "metadata": {},
   "source": [
    "# LSTM-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "\n",
    "# Encoder\n",
    "lstmlstm = Sequential(name='LSTM-LSTM')\n",
    "lstmlstm.add(LSTM(100, activation='relu',  input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "lstmlstm.add(RepeatVector(n_steps))\n",
    "lstmlstm.add(Dropout(.2))\n",
    "\n",
    "# Decoder\n",
    "lstmlstm.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "lstmlstm.add(TimeDistributed(Sequential([\n",
    "    Dropout(.2, name='Dropout_1'),\n",
    "    Dense(100, name='Dense_1'),\n",
    "    Dropout(.2, name='Dropout_2'),\n",
    "    Dense(n_steps, name='Dense_2')\n",
    "])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f021a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model(lstmlstm, time_distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef913c",
   "metadata": {},
   "source": [
    "# LSTM-LSTM - short training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01542dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "\n",
    "# Encoder\n",
    "lstmlstm_short = Sequential(name='LSTM-LSTM')\n",
    "lstmlstm_short.add(LSTM(100, activation='relu',  input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "lstmlstm_short.add(RepeatVector(n_steps))\n",
    "lstmlstm_short.add(Dropout(.2))\n",
    "\n",
    "# Decoder\n",
    "lstmlstm_short.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "lstmlstm_short.add(TimeDistributed(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d559d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_lags, n_steps, lead_time, test_size = 11, 11, 0, .2\n",
    "\n",
    "epochs, batch_size, verbose = 100, 72, 0\n",
    "\n",
    "model_params = {'callbacks':[EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    mode=\"auto\")]}\n",
    "\n",
    "\n",
    "data1 = data.loc['2022-05-15':,:].drop(columns=['neg_median','last','median'])\n",
    "\n",
    "X_train, X_val, y_train, y_val, last_row = prepare_data(data1, 'btc', n_lags, n_steps, lead_time, test_size)\n",
    "\n",
    "fit_model(lstmlstm_short, time_distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf5750",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "# Encoder\n",
    "cnn_lstm = Sequential(name='CNN-LSTM')\n",
    "cnn_lstm.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "cnn_lstm.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_lstm.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm.add(Flatten())\n",
    "cnn_lstm.add(RepeatVector(n_steps))\n",
    "\n",
    "# Decoder\n",
    "cnn_lstm.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "cnn_lstm.add(TimeDistributed(output))\n",
    "\n",
    "\n",
    "epochs, batch_size, verbose = 300, 32, 0\n",
    "\n",
    "\n",
    "fit_model(cnn_lstm, epochs=epochs, batch_size=batch_size, time_distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0bd68",
   "metadata": {},
   "source": [
    "# CNN-Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c086dc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "# Encoder\n",
    "bi_lstm = Sequential(name='CNN-LSTM')\n",
    "bi_lstm.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bi_lstm.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "bi_lstm.add(MaxPooling1D(pool_size=2))\n",
    "bi_lstm.add(Flatten())\n",
    "bi_lstm.add(RepeatVector(n_steps))\n",
    "\n",
    "# Decoder\n",
    "bi_lstm.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bi_lstm.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "bi_lstm.add(TimeDistributed(output))\n",
    "\n",
    "\n",
    "epochs, batch_size, verbose = 300, 32, 0\n",
    "\n",
    "\n",
    "fit_model(cnn_lstm, epochs=epochs, batch_size=batch_size, time_distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da80f8",
   "metadata": {},
   "source": [
    "# Attention and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
